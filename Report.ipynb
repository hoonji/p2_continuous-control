{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530268fa",
   "metadata": {},
   "source": [
    "# Project 2: Continuous Control\n",
    "\n",
    "## Overview\n",
    "\n",
    "The project solves the multi agent Reacher environment using the PPO algorithm [1].\n",
    "\n",
    "The PPO code draws heavily from Costa Huang's continuous action PPO implementation for the Gymnasium Half-Cheetah Environment [2].\n",
    "\n",
    "My original work on the PPO code includes the following:\n",
    "- Modified to support unityagents multi agent Reacher environment\n",
    "- Hyperparameter tuning\n",
    "- Updated network architecture\n",
    "- Removed non-essential code\n",
    "- Simplified score recording\n",
    "- Added model checkpointing\n",
    "\n",
    "## Learning Algorithm\n",
    "\n",
    "### Background\n",
    "\n",
    "The project uses Proximal Policy Optimization [1], which maximizes the surrogate objective function\n",
    "$$\\sum_tE_{s_t \\sim p_{\\theta}(s_t)}[E_{a_t \\sim \\pi_{\\theta}(a_t|s_t)}[\\frac{\\pi_{\\theta'}(a_t|s_t)}{\\pi_{\\theta}(a_t|s_t)}\\gamma^tA^{\\pi_{\\theta}}(s_t,a_t)]]$$\n",
    "which is roughly equivalent to maximizing the reinforcement learning objective [3]\n",
    "$$E_{\\tau \\sim p_\\theta(\\tau)}[\\sum_{t}\\gamma^tr(s_t,a_t)]$$\n",
    "given that $\\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta}(a|s)} \\le \\epsilon$ [4].\n",
    "\n",
    "PPO keeps the new policies similar to the old by conditionallly clipping the ratio $\\frac{\\pi_{\\theta'}(a|s)}{\\pi_{\\theta}(a|s)}$:\n",
    "$$\\min(\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_k}(a|s)}A^{\\pi_{\\theta_k}}(s,a), clip(\\frac{\\pi_\\theta(a|s)}{\\pi_{\\theta_k}(a|s)},1-\\epsilon, 1+\\epsilon)A^{\\pi_{\\theta_k}}(s,a))$$\n",
    "\n",
    "SGD is performed on the above objective with respect to model parameters θ to maximize the agent's performance.\n",
    "\n",
    "### Implementation details\n",
    "\n",
    "This particular PPO implementation [2] contains extensions that improve the performance of the vanilla PPO algorithm:\n",
    "- Orthogonal initialization of weights [5]\n",
    "- Generalized Advanatage Estimation [6]\n",
    "- Mini batch updates (prevents overfitting to local minima)\n",
    "- Global gradient clipping [6]\n",
    "\n",
    "## Hyperparameters\n",
    "\n",
    "```\n",
    "LEARNING_RATE = 3e-4\n",
    "ADAM_EPS = 1e-5\n",
    "TOTAL_TIMESTEPS = 4000000\n",
    "GAMMA = .99\n",
    "LAMBDA = .95\n",
    "UPDATE_EPOCHS = 10\n",
    "N_MINIBATCHES = 32\n",
    "CLIP_COEF = .2\n",
    "MAX_GRAD_NORM = 5\n",
    "GAE_LAMBDA = .95\n",
    "V_COEF = .5\n",
    "ENT_COEF = .01\n",
    "HIDDEN_LAYER_SIZE = 512\n",
    "ANNEAL_LR = False\n",
    "ROLLOUT_LEN = 2048\n",
    "```\n",
    "\n",
    "Differences from the original PPO implementation [2]:\n",
    "- Total timesteps increased: 2e6 -> 4e6\n",
    "- Max grad norm increased: .5 -> 5\n",
    "- Entropy coefficient increased: 0 -> .01\n",
    "- Hidden layer size increased: 64 -> 512\n",
    "- Learning rate annealing disabled (it had no positive effect on performance)\n",
    "- Batch size increased: 2048 -> 2048 * 20\n",
    "\n",
    "## Model architecture\n",
    "\n",
    "The model consists of two networks, one actor network and one critic network, that both consist of three fully connected layers, with hidden layers having size 512.\n",
    "\n",
    "The actor network takes in inputs of size (n_batch, n_observations) and outputs values of size (n_batch, n_actions) with each value between -1 and 1. The values represent the means of each of the four action components and are used to sample action values from a normal distribution. The actor network uses ReLU activations for the initial layers to maximize learning stability, and uses a tanh output activation to scale values between -1 and 1.\n",
    "\n",
    "The critic network also takes in inputs of size (n_batch, n_observations), but outputs values of size (n_batch, 1) that represent the predicted value of the observation. The critic net also uses ReLU activations between layers and does not require the last tanh layer.\n",
    "\n",
    "For convenience in training code, the network module provides a helper function `get_action_and_value` that returns the sampled action, log probabilities of the action, entropies of the action distributions, and predicted values from the critic.\n",
    "\n",
    "## Learning curve\n",
    "\n",
    "The agent achieved an average score of 30 in less than 1500 total episodes.\n",
    "\n",
    "![](learning_curve.png)\n",
    "\n",
    "## Future work\n",
    "\n",
    "- Different network architectures and network sizes can be experimented with to find the best fit for the current problem. Batch normalization may also help stabilize learning.\n",
    "- The agents' performances peak at below 38. Perhaps the entropy coefficient can be annealed to 0 to reduce exploration in the later stages of learning and thereby improve peak performance.\n",
    "- Observation and reward normalization can also help to stabilize training performance\n",
    "\n",
    "## References\n",
    "\n",
    "- [1] https://arxiv.org/pdf/1707.06347.pdf Schulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. \"Proximal policy optimization algorithms.\" arXiv preprint arXiv:1707.06347 (2017).\n",
    "- [2] https://github.com/vwxyzjn/ppo-implementation-details/blob/main/ppo_continuous_action.py\n",
    "- [3] https://www.youtube.com/watch?v=ySenCHPsKJU&list=PL_iWQOsE6TfX7MaC6C3HcdOf1g337dlC9&index=38&ab_channel=RAIL Berkeley CS285 Lecture 9, Part 1\n",
    "- [4] https://www.youtube.com/watch?v=ySenCHPsKJU&list=PL_iWQOsE6TfX7MaC6C3HcdOf1g337dlC9&index=38&ab_channel=RAIL Berkeley CS285 Lecture 9, Part 2\n",
    "- [5] https://openreview.net/forum?id=r1etN1rtPB Logan, Engstrom, Ilyas Andrew, Santurkar Shibani, Tsipras Dimitris, Janoos Firdaus, Rudolph Larry, and Madry Aleksander. \"Implementation matters in deep RL: A case study on PPO and TRPO.\" In International Conference on Learning Representations. 2019.\n",
    "- [6] https://openreview.net/forum?id=nIAxjsniDzg Andrychowicz, Marcin, Anton Raichuk, Piotr Stańczyk, Manu Orsini, Sertan Girgin, Raphaël Marinier, Leonard Hussenot et al. \"What matters for on-policy deep actor-critic methods? a large-scale study.\" In International conference on learning representations. 2021."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
